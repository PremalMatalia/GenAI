{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7af194b-3b87-43ef-9a21-dc3bc4ee01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages quietly (-q flag suppresses output):\n",
    "# - langchain: Framework for developing applications with LLMs\n",
    "# - langchain_experimental: Experimental components for LangChain\n",
    "# - openai: OpenAI API client library\n",
    "# - python-dotenv: Library to load environment variables from .env files\n",
    "# - Langchain_openai: LangChain integration with OpenAI\n",
    "!pip install -q langchain langchain_experimental openai python-dotenv Langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "phantom-period",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found .env file at: D:\\AI & ML\\Jupyter Notebooks\\GenAI\\.env\n",
      "\n",
      "Environment variables loaded from .env file:\n",
      "OPENAI_API_KEY=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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "# Find the .env file that would be loaded\n",
    "dotenv_path = find_dotenv()\n",
    "\n",
    "if dotenv_path:\n",
    "    print(f\"Found .env file at: {os.path.abspath(dotenv_path)}\")\n",
    "    \n",
    "    # Load the .env file\n",
    "    load_dotenv(dotenv_path)\n",
    "    \n",
    "    # Optionally, print the contents (be careful with sensitive data)\n",
    "    print(\"\\nEnvironment variables loaded from .env file:\")\n",
    "    with open(dotenv_path, 'r') as f:\n",
    "        # Print each line but mask values for lines containing sensitive keywords\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#'):\n",
    "                if any(keyword in line.lower() for keyword in ['OPENAI_API_KEY', 'password', 'secret', 'token']):\n",
    "                    key = line.split('=')[0]\n",
    "                    print(f\"{key}=********\")\n",
    "                else:\n",
    "                    print(line)\n",
    "else:\n",
    "    print(\"No .env file found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sustained-rugby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key value found: 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\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the OpenAI API key from environment variables\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "# Validate that the API key exists\n",
    "if api_key is None:\n",
    "    # Raise an error if the API key is not found\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable is not set. Please check your .env file.\")\n",
    "else:\n",
    "    # Print confirmation that the key was found (with value)\n",
    "    #print(f\"key value found: {api_key}\")    \n",
    "\n",
    "# Set the API key in the environment variables for use by OpenAI libraries\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "upset-simpson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env file created at: D:\\AI & ML\\Jupyter Notebooks\\GenAI\\.env\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "\n",
    "# Define the path for your .env file in the current working directory\n",
    "# instead of using __file__ which isn't available in Jupyter notebooks\n",
    "env_path = os.path.join(os.getcwd(), '.env')\n",
    "\n",
    "# Example content for your .env file\n",
    "env_content = \"\"\"\n",
    "# OpenAI API Configuration\n",
    "OPENAI_API_KEY=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\n",
    "\n",
    "# Add other environment variables as needed\n",
    "# DATABASE_URL=your_database_connection_string\n",
    "# DEBUG=True\n",
    "\"\"\"\n",
    "\n",
    "# Write the .env file\n",
    "with open(env_path, 'w') as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "print(f\".env file created at: {env_path}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a19cac01-460d-456a-8956-3eb1b937ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from a .env file (typically contains API keys)\n",
    "#load_dotenv()\n",
    "# Initialize a ChatOpenAI instance with specific parameters:\n",
    "# - Using the gpt-4o-mini model\n",
    "# - Setting maximum response length to 1000 tokens\n",
    "# - Setting temperature to 0 for deterministic/consistent outputs\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=1000, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddbf3e0-70c7-4d6e-b86a-edd4ed2254fb",
   "metadata": {},
   "source": [
    "<h3> Create a simple in-memory store for chat histories </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c73426ec-a34b-452f-aa10-7c57bcab1a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_chat_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a9ae6-eaa1-40b6-9918-5f4d448e0476",
   "metadata": {},
   "source": [
    "<h3>Create the prompt template</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef4eff9b-a671-4893-ab49-32ed31fe191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat prompt template with:\n",
    "# 1. A system message defining the assistant's role\n",
    "# 2. A placeholder for chat history from previous interactions\n",
    "# 3. A human message template that will be filled with user input\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful AI assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\",\"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7fc69d-e56b-4a61-b3f9-38dc78f17038",
   "metadata": {},
   "source": [
    "<h3> Combine the prompt and model into a runnable chain </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d26685b-159c-401f-9519-0e715b924c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caece78-fb56-4650-8250-c6efabf0bdf6",
   "metadata": {},
   "source": [
    "<h3> Wrap the chain with message history </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2cf746e1-823d-4ad5-a74f-72c8e8c7a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a runnable chain that maintains conversation history\n",
    "# - chain: The base chain to execute\n",
    "# - get_chat_history: Function that retrieves chat history for a session\n",
    "# - input_messages_key: Key in the input dictionary that contains the user's message\n",
    "# - history_messages_key: Key in the input dictionary where chat history will be stored\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff909e21-ce87-4118-bc4a-13eb04c5c8e2",
   "metadata": {},
   "source": [
    "<h4> Example usage </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a39ce6b1-65bc-4208-9177-d79e5dc70687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Please check your OpenAI API quota at: https://platform.openai.com/account/usage\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "session_id = \"premal_123\"\n",
    "\n",
    "try:\n",
    "    response1 = chain_with_history.invoke(\n",
    "        {\"input\": \"Hello! How are you?\"},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    print(\"AI:\", response1.content)\n",
    "    \n",
    "    # Add delay between requests to avoid rate limiting\n",
    "    time.sleep(60)  # Wait for 3 seconds between requests\n",
    "    \n",
    "    response2 = chain_with_history.invoke(\n",
    "        {\"input\": \"What was my previous message?\"},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    print(\"AI:\", response2.content)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    print(\"Please check your OpenAI API quota at: https://platform.openai.com/account/usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "be87f1cd-23ad-47e2-9582-5be63dbf9820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversation History:\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nConversation History:\")\n",
    "for message in store[session_id].messages:\n",
    "    print(f\"{message.type}: {message.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (news)",
   "language": "python",
   "name": "news"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
